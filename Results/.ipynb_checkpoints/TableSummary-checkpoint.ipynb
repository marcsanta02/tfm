{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bb23018-cf03-4a44-8892-c312a5cd15c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost.callback import EarlyStopping\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce7b00fe-6a94-472e-87e0-848242fdbadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAT_COLS = ['type', 'subtype', 'size_category', 'temperature', 'above_median_access_count', 'above_median_filesize']\n",
    "NUM_COLS = ['stddev_access_date',\n",
    " 'dt_last_access_date',\n",
    " 'dt_second_last_access_date',\n",
    " 'dt_third_last_access_date',\n",
    " 'dt_fourth_last_access_date',\n",
    " 'dt_fifth_last_access_date',\n",
    " 'normalized_access_count',\n",
    " 'normalized_filesize',\n",
    " 'access_count_last_1_day',\n",
    " 'access_count_last_3_days',\n",
    " 'access_count_last_7_days',\n",
    " 'access_count_last_15_days',\n",
    " 'lifetime',\n",
    " 'access_count',\n",
    " 'read_data_per_second']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d859a9f-9fcd-414b-add3-9cd7b439afb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/data/astro/scratch/msantama/tfm/data3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8fea5dc-918c-46b4-b776-123533e6c3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will predict for 10 days into the future, so we will drop the columns that are not needed\n",
    "df.drop(columns=['m_date_window'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1116332-bcde-4c87-b653-92c629f9810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[CAT_COLS] = df[CAT_COLS].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b00ee98-ab29-46ec-b7bc-b0827958c3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_cols = [\n",
    "    'dt_last_access_date', 'dt_second_last_access_date',\n",
    "    'dt_third_last_access_date', 'dt_fourth_last_access_date',\n",
    "    'dt_fifth_last_access_date', 'lifetime'\n",
    "]\n",
    "for col in dt_cols:\n",
    "    df[f'log_{col}'] = np.log1p(df[col])\n",
    "df.drop(columns=dt_cols, inplace=True, errors=\"ignore\")\n",
    "access_cols = [\n",
    "    'access_count_last_1_day', 'access_count_last_3_days',\n",
    "    'access_count_last_7_days', 'access_count_last_15_days'\n",
    "]\n",
    "df[access_cols] = df[access_cols].fillna(0)\n",
    "\n",
    "df['ratio_1d_15d'] = df['access_count_last_1_day'] / (df['access_count_last_15_days'] + 1)\n",
    "df['ratio_3d_15d'] = df['access_count_last_3_days'] / (df['access_count_last_15_days'] + 1)\n",
    "df['ratio_1d_3d'] = df['access_count_last_1_day'] / (df['access_count_last_3_days'] + 1)\n",
    "df['ratio_3d_7d'] = df['access_count_last_3_days'] / (df['access_count_last_7_days'] + 1)\n",
    "\n",
    "df['access_trend'] = (\n",
    "    df['access_count_last_1_day'] - df['access_count_last_3_days']/3 +\n",
    "    df['access_count_last_3_days'] - df['access_count_last_7_days']/7\n",
    ")\n",
    "# Ordered mapping (ensure the order is consistent with your definitions)\n",
    "temperature_map = {'cold': 0, 'cold-warm': 1, 'warm-hot': 2, 'hot': 3}\n",
    "size_category_map = {'small': 0, 'medium': 1, 'large': 2, 'xlarge': 3}\n",
    "\n",
    "df['temperature_encoded'] = df['temperature'].map(temperature_map)\n",
    "df['size_category_encoded'] = df['size_category'].map(size_category_map)\n",
    "\n",
    "df.drop(columns=['temperature', 'size_category'], inplace=True, errors='ignore')\n",
    "df['type_binary'] = (df['type'] == 'mc').astype(int)\n",
    "df.drop(columns='type', inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c6812d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Rows (instances)     Unique files  \\\n",
      "Split                                                              \n",
      "Train (rolling 13)            4005560 ± 1419392  468342 ± 148563   \n",
      "Validation (last)               288325 ± 114117  285196 ± 110446   \n",
      "Test (next)                     288541 ± 114022  285412 ± 110354   \n",
      "Total bytes (Train∪Val∪Test)                                       \n",
      "\n",
      "                                 Time span        Pos ratio (y=1)  \n",
      "Split                                                              \n",
      "Train (rolling 13)            14.00 ± 0.00        0.0764 ± 0.0212  \n",
      "Validation (last)              1.00 ± 0.00        0.0791 ± 0.0365  \n",
      "Test (next)                    1.00 ± 0.00        0.0790 ± 0.0366  \n",
      "Total bytes (Train∪Val∪Test)                1.679e+15 ± 4.423e+14  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, deque\n",
    "\n",
    "def aggregate_metrics_fast(df: pd.DataFrame, train_len: int = 13, start_min: int = 13):\n",
    "    \"\"\"\n",
    "    Efficient rolling computation of split metrics:\n",
    "      - Train: periods [s .. s+train_len]\n",
    "      - Val:   period  (s+train_len)\n",
    "      - Test:  period  (s+train_len+1)\n",
    "    Returns a DataFrame with mean ± std across all valid starts.\n",
    "    Required columns in df: period, pnfsid, size, y1\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Precompute period-level summaries ---\n",
    "    # Sort periods to get a dense, ordered axis\n",
    "    periods = np.sort(df['period'].unique())\n",
    "    p_to_idx = {p: i for i, p in enumerate(periods)}\n",
    "    pmin, pmax = periods.min(), periods.max()\n",
    "\n",
    "    # Per-period rows & positive labels\n",
    "    per_period = df.groupby('period').agg(\n",
    "        rows_in_period=('y1', 'size'),\n",
    "        pos_in_period=('y1', 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Per-period set of files\n",
    "    files_by_period = (\n",
    "        df.groupby('period')['pnfsid']\n",
    "          .apply(lambda s: set(s.values))\n",
    "          .to_dict()\n",
    "    )\n",
    "\n",
    "    # Global size per file (max, in case duplicates exist)\n",
    "    size_by_file = df.groupby('pnfsid')['size'].max().to_dict()\n",
    "\n",
    "    # Valid starts: need val at end = s+train_len, and test at end+1\n",
    "    # i.e., s must satisfy s+train_len+1 ≤ pmax\n",
    "    # Also enforce s ≥ start_min\n",
    "    valid_starts = [s for s in periods if (s >= start_min) and (s + train_len + 1 <= pmax)]\n",
    "    if not valid_starts:\n",
    "        raise ValueError(\"No valid starts for the given train_len and data span.\")\n",
    "\n",
    "    # --- Helpers to maintain rolling unions efficiently ---\n",
    "    def add_period_files(counter: Counter, files: set, size_by_file: dict) -> int:\n",
    "        \"\"\"Increment counts for files; return bytes added when a file goes 0->1.\"\"\"\n",
    "        added_bytes = 0\n",
    "        for f in files:\n",
    "            prev = counter.get(f, 0)\n",
    "            counter[f] = prev + 1\n",
    "            if prev == 0:\n",
    "                added_bytes += size_by_file.get(f, 0)\n",
    "        return added_bytes\n",
    "\n",
    "    def remove_period_files(counter: Counter, files: set, size_by_file: dict) -> int:\n",
    "        \"\"\"Decrement counts for files; return bytes removed when a file goes 1->0.\"\"\"\n",
    "        removed_bytes = 0\n",
    "        for f in files:\n",
    "            prev = counter.get(f, 0)\n",
    "            if prev <= 1:\n",
    "                # going to 0\n",
    "                if prev == 1:\n",
    "                    removed_bytes += size_by_file.get(f, 0)\n",
    "                counter.pop(f, None)\n",
    "            else:\n",
    "                counter[f] = prev - 1\n",
    "        return removed_bytes\n",
    "\n",
    "    # --- Rolling accumulators for metrics across starts ---\n",
    "    # We collect per-start metrics for Train/Val/Test and for union bytes across all splits\n",
    "    coll = {\n",
    "        \"Train\": {\"rows\": [], \"uniq_files\": [], \"pos_ratio\": [], \"unique_periods\": []},\n",
    "        \"Validation\": {\"rows\": [], \"uniq_files\": [], \"pos_ratio\": [], \"unique_periods\": []},\n",
    "        \"Test\": {\"rows\": [], \"uniq_files\": [], \"pos_ratio\": [], \"unique_periods\": []},\n",
    "        \"TotalBytesUnion\": []  # Train ∪ Val ∪ Test bytes per start\n",
    "    }\n",
    "\n",
    "    # --- Initialize rolling Train window (first start) ---\n",
    "    # We'll maintain:\n",
    "    #  - running sums of rows and positives for Train via arrays + prefix sums\n",
    "    #  - train Counter/set union for unique files and bytes\n",
    "    # Val/Test are single periods: compute on the fly per start.\n",
    "    rows_map = dict(zip(per_period['period'], per_period['rows_in_period']))\n",
    "    pos_map  = dict(zip(per_period['period'], per_period['pos_in_period']))\n",
    "\n",
    "    # Prefix sums for rows/pos to get Train rows/pos quickly\n",
    "    rows_arr = np.array([rows_map.get(p, 0) for p in periods], dtype=np.int64)\n",
    "    pos_arr  = np.array([pos_map.get(p, 0) for p in periods], dtype=np.int64)\n",
    "    rows_cum = np.cumsum(rows_arr)\n",
    "    pos_cum  = np.cumsum(pos_arr)\n",
    "\n",
    "    def sum_range(cum, i, j):\n",
    "        # inclusive i..j on periods index\n",
    "        if i > j: return 0\n",
    "        return cum[j] - (cum[i-1] if i > 0 else 0)\n",
    "\n",
    "    # Train union (files) rolling\n",
    "    train_counts = Counter()\n",
    "    train_union_bytes = 0\n",
    "    train_window = deque()  # store the period values currently in Train\n",
    "\n",
    "    first_start = valid_starts[0]\n",
    "    first_end = first_start + train_len\n",
    "\n",
    "    # Seed the Train window with periods [first_start..first_end]\n",
    "    seed_periods = [p for p in periods if (first_start <= p <= first_end)]\n",
    "    for p in seed_periods:\n",
    "        train_union_bytes += add_period_files(train_counts, files_by_period.get(p, set()), size_by_file)\n",
    "        train_window.append(p)\n",
    "\n",
    "    # --- Iterate over starts (rolling) ---\n",
    "    for s in valid_starts:\n",
    "        end = s + train_len\n",
    "        val_p = end\n",
    "        test_p = end + 1\n",
    "\n",
    "        # Roll Train window if we advanced s\n",
    "        if s != train_window[0]:\n",
    "            # Remove leftmost period(s) until left == s\n",
    "            while train_window and train_window[0] < s:\n",
    "                p_out = train_window.popleft()\n",
    "                train_union_bytes -= remove_period_files(train_counts, files_by_period.get(p_out, set()), size_by_file)\n",
    "            # Add new right edge periods until right == end\n",
    "            while (not train_window) or (train_window[-1] < end):\n",
    "                p_in = (train_window[-1] + 1) if train_window else s\n",
    "                train_union_bytes += add_period_files(train_counts, files_by_period.get(p_in, set()), size_by_file)\n",
    "                train_window.append(p_in)\n",
    "\n",
    "        # TRAIN metrics (rows/pos via prefix sums; uniq files via counter)\n",
    "        i = p_to_idx[s]\n",
    "        j = p_to_idx[end]\n",
    "        train_rows = int(sum_range(rows_cum, i, j))\n",
    "        train_pos  = int(sum_range(pos_cum,  i, j))\n",
    "        train_pos_ratio = (train_pos / train_rows) if train_rows > 0 else np.nan\n",
    "        train_unique_files = len(train_counts)\n",
    "        train_unique_periods = len(train_window)  # observed span in periods\n",
    "\n",
    "        coll[\"Train\"][\"rows\"].append(train_rows)\n",
    "        coll[\"Train\"][\"uniq_files\"].append(train_unique_files)\n",
    "        coll[\"Train\"][\"pos_ratio\"].append(train_pos_ratio)\n",
    "        coll[\"Train\"][\"unique_periods\"].append(train_unique_periods)\n",
    "\n",
    "        # VALIDATION (single period = end)\n",
    "        val_files = files_by_period.get(val_p, set())\n",
    "        val_rows = rows_map.get(val_p, 0)\n",
    "        val_pos  = pos_map.get(val_p, 0)\n",
    "        val_pos_ratio = (val_pos / val_rows) if val_rows > 0 else np.nan\n",
    "        coll[\"Validation\"][\"rows\"].append(val_rows)\n",
    "        coll[\"Validation\"][\"uniq_files\"].append(len(val_files))\n",
    "        coll[\"Validation\"][\"pos_ratio\"].append(val_pos_ratio)\n",
    "        coll[\"Validation\"][\"unique_periods\"].append(1)\n",
    "\n",
    "        # TEST (single period = end+1)\n",
    "        test_files = files_by_period.get(test_p, set())\n",
    "        test_rows = rows_map.get(test_p, 0)\n",
    "        test_pos  = pos_map.get(test_p, 0)\n",
    "        test_pos_ratio = (test_pos / test_rows) if test_rows > 0 else np.nan\n",
    "        coll[\"Test\"][\"rows\"].append(test_rows)\n",
    "        coll[\"Test\"][\"uniq_files\"].append(len(test_files))\n",
    "        coll[\"Test\"][\"pos_ratio\"].append(test_pos_ratio)\n",
    "        coll[\"Test\"][\"unique_periods\"].append(1)\n",
    "\n",
    "        # TOTAL BYTES over Train ∪ Val ∪ Test (dedup pnfsid)\n",
    "        # Efficiently: overlay Val/Test files on current Train counter without mutating it\n",
    "        # We simulate increments to count 0->1 to measure extra bytes, then revert.\n",
    "        delta_bytes = 0\n",
    "        temp_adds = []\n",
    "        for f in val_files:\n",
    "            if train_counts.get(f, 0) == 0:\n",
    "                delta_bytes += size_by_file.get(f, 0)\n",
    "                temp_adds.append(f)\n",
    "        for f in test_files:\n",
    "            # If also in val temp adds, don't double-add; but if in Train already, skip\n",
    "            if (train_counts.get(f, 0) == 0) and (f not in temp_adds):\n",
    "                delta_bytes += size_by_file.get(f, 0)\n",
    "\n",
    "        total_bytes_union = train_union_bytes + delta_bytes\n",
    "        coll[\"TotalBytesUnion\"].append(total_bytes_union)\n",
    "\n",
    "    # --- Aggregate mean ± std over starts ---\n",
    "    def mean_std_fmt(a, kind=\"int\"):\n",
    "        a = np.array(a, dtype=float)\n",
    "        m = np.nanmean(a)\n",
    "        s = np.nanstd(a, ddof=1) if len(a) > 1 else 0.0\n",
    "        if kind == \"int\":\n",
    "            return f\"{int(round(m))} ± {int(round(s))}\"\n",
    "        if kind == \"pct\":\n",
    "            return f\"{m:.4f} ± {s:.4f}\"\n",
    "        if kind == \"float2\":\n",
    "            return f\"{m:.2f} ± {s:.2f}\"\n",
    "        if kind == \"bytes\":\n",
    "            return f\"{m:.3e} ± {s:.3e}\"\n",
    "        return f\"{m:.3f} ± {s:.3f}\"\n",
    "\n",
    "    def pack(split):\n",
    "        return {\n",
    "            \"Rows (instances)\": mean_std_fmt(coll[split][\"rows\"], \"int\"),\n",
    "            \"Unique files\":     mean_std_fmt(coll[split][\"uniq_files\"], \"int\"),\n",
    "            \"Time span\":        mean_std_fmt(coll[split][\"unique_periods\"], \"float2\"),  # periods\n",
    "            \"Pos ratio (y=1)\":  mean_std_fmt(coll[split][\"pos_ratio\"], \"pct\"),\n",
    "        }\n",
    "\n",
    "    out = pd.DataFrame(\n",
    "        [\n",
    "            {\"Split\": \"Train (rolling 13)\",     **pack(\"Train\")},\n",
    "            {\"Split\": \"Validation (last)\",      **pack(\"Validation\")},\n",
    "            {\"Split\": \"Test (next)\",            **pack(\"Test\")},\n",
    "            {\"Split\": \"Total bytes (Train∪Val∪Test)\", \n",
    "             \"Rows (instances)\": \"\",\n",
    "             \"Unique files\":     \"\",\n",
    "             \"Time span\":        \"\",\n",
    "             \"Pos ratio (y=1)\":  mean_std_fmt(coll[\"TotalBytesUnion\"], \"bytes\")}\n",
    "        ]\n",
    "    ).set_index(\"Split\")\n",
    "\n",
    "    return out\n",
    "\n",
    "# ---- Usage ----\n",
    "metrics_df = aggregate_metrics_fast(df, train_len=13, start_min=13)\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad6b161-c096-4166-a682-bd3d0b53c578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML env",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
